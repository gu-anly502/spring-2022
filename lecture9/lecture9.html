<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Lecture 9 - Machine Learning with Spark</title>
    <meta charset="utf-8" />
    <meta name="author" content="Anderson Monken" />
    <script src="libs/header-attrs-2.11/header-attrs.js"></script>
    <link href="libs/remark-css-0.0.1/default.css" rel="stylesheet" />
    <link href="libs/remark-css-0.0.1/default-fonts.css" rel="stylesheet" />
    <link href="libs/tachyons-4.12.0/tachyons.min.css" rel="stylesheet" />
    <link href="libs/panelset-0.2.6/panelset.css" rel="stylesheet" />
    <script src="libs/panelset-0.2.6/panelset.js"></script>
    <link href="libs/xaringanExtra-extra-styles-0.2.6/xaringanExtra-extra-styles.css" rel="stylesheet" />
    <link href="libs/tile-view-0.2.6/tile-view.css" rel="stylesheet" />
    <script src="libs/tile-view-0.2.6/tile-view.js"></script>
    <script src="libs/clipboard-2.0.6/clipboard.min.js"></script>
    <link href="libs/shareon-1.4.1/shareon.min.css" rel="stylesheet" />
    <script src="libs/shareon-1.4.1/shareon.min.js"></script>
    <link href="libs/xaringanExtra-shareagain-0.2.6/shareagain.css" rel="stylesheet" />
    <script src="libs/xaringanExtra-shareagain-0.2.6/shareagain.js"></script>
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# Lecture 9 - Machine Learning with Spark
## ANLY 502 - Big Data and Cloud Computing
### Anderson Monken
### Spring 2022

---












.pull-left[
## Looking Back

* Linux, Git, OS
* Parallelization, MapReduce
* Hadoop, HDFS
* Spark, RDDs, DataFrames

## Upcoming

* Spark ML
* Spark NLP
* Spark Streaming
* Wider Cluster World!

]

.pull-right[

## Admin

* AWS Academy Courses
* Grade Reminders
* Deadlines
* Review of Labs and HW

## Today

* Project Data Access
* Overview of ML capabilities with Spark
* ML Pipelines
* ML Models and Tuning

]


---

## AWS Academy

* Credit limit - $100
* Course numbers:

  * Course #1 - 12142
  * Course #2 - 12143
  * Course #3 - 12144
  * Course #4 - 12147 (not assigned yet)
  
STAY CURRENT COURSE UNLESS YOU HAVE RUN OUT OF CREDITS OR &gt;$90 USED! 

Note that you will have to repeat several setup steps:

- security group
- EC2 keypair uploading (the AWS part only)
- sagemaker setup
- any S3 uploading or copying as well as bucket creation as necessary
- EMR configuration (you will know what this means soon)

---

## Grades

- You are responsible for your grades

- If there is a problem, you need to bring it up

- All assignments are mandatory! No dropping assignments.

- State of assignments from syllabus

Total points: 1000

- Project Portfolio: 350 points (35%)
    - Intermediate Portfolio Assignments: 210 points (21%)
    - Final Portfolio: 90 points (9%)
    - Presentation: 50 points (5%)
 -HW Assignments: 200 points (20%) - Assignments 1 through 7
 -Discussion Posts: 200 points (20%) - Multiple project posts to go
 -Lab Deliverables: 200 points (20%) - Halfway through
 -Participation/Quizzes: 50 points (5%) - Halfway through

---

## Deadlines

- Pre-Lecture Reading - Due by Lecture 3/16 6 pm - https://georgetown.instructure.com/courses/142215/assignments/719693
- Lab 7 Deliverable - Due by 3/17 11:59 pm (+24 hr penalty-free) - https://georgetown.instructure.com/courses/142215/assignments/712899 
- Homework 7 - Due by 3/19 (extending one more day!) 11:59 pm (+24 hr penalty-free) - https://georgetown.instructure.com/courses/142215/assignments/712891
- Project Discussion Posts - Due by 3/17 11:59pm (+24 hour penalty-free) - https://georgetown.instructure.com/courses/142215/discussion_topics/819538
    
---

## Live review of solution for Lab 6 and HW 6

https://github.com/bigdatateaching/lab-intro-spark/
https://github.com/bigdatateaching/assignment-intro-spark/

---

## Questions about Lab 7 and HW 7

[discussion thread](https://georgetown.instructure.com/courses/142215/discussion_topics/819189)

---

## Reddit project data

Notebook that prepares the data!

Major features:
- error handling
- consistent data schema
- S3 interactions
- keeping computation with the data

---

## Accessing your project data

[S3 bucket](https://s3.console.aws.amazon.com/s3/buckets/bigdatateaching?region=us-east-1&amp;prefix=reddit/subreddits/&amp;showversions=false)

--

Copy your reddit data from the central bucket to your personal bucket

```
aws s3 cp s3://bigdatateaching/reddit/subreddits/[[YOUR SUBREDDIT]]/ s3://bigdatarepo/reddit --recursive
```

--

What types of commands should you be making on this data?

--

```
df = spark.read.parquet('s3://bigdatarepo/reddit')

df.count()

df_small = df.limit(10000)

df_small.show(10)

df_pd = df_small.toPandas()

df_pd.head()

............

```



---

class: inverse, center, middle

# Assumption: knowledge of ML techniques and process

---

## Overview of ML

.center[
&lt;img src="img/ml_map.png" width=800&gt;
]

---

## The advanced analytics/ML process

.pull-left[
1. Gather and collect data

1. Clean and inspect data

1. Perform feature engineering

1. Split data into train/test

1. Evaluate and compare models

]


.pull-right[
&lt;img src="img/spdg_2401.png"&gt;
]


---

class: inverse, center, middle

# ML with Spark

---

## What is `MLlib`

.pull-left[
#### Capabilities

- Gather and clean data
- Perform feature engineering
- Perform feature selection
- Train and tune models
- Put models in production

#### API is divided into two packages

`org.apache.spark.ml` (High level API)
- Provides higher level API built on top of `DataFrames`
- Allows the construction of ML _pipelines_

`org.apache.spark.mllib` (Predates `DataFrames`)
- Original API built on top of RDDs
]

.pull-right[
#### Inspired by `scikit-learn`

* `DataFrame`

* `Transformer`

* `Estimator`

* `Pipeline`

* `Parameter`
]


---


class: inverse

# Transformers, esimators, and pipelines

.center[
&lt;img src="img/spdg_2402.png" width=800&gt;
]

---

## Transformers

.pull-left[
`Transformers` take `DataFrames` as _input_, and return a **new DataFrame** as output. `Transformers` do not learn any parameters from the data, they simply apply rule-based transformations to either **prepare data for model training** or **generate predictions using a trained model.**  

`Transformers` are run using the `.transform()` method
]

.pull-right[
&lt;img src="img/spdg_2403.png"&gt;
]

---

## Some Examples of Transformers

* Converting categorical variables to numeric (must do this)
  * `StringIndexer`
  * `OneHotEncoder` can act on multiple columns at a time 

* Data Normalization
  * `Normalizer`
  * `StandardScaler`

* String Operations
  * `Tokenizer`
  * `StopWordsRemover`
  * `PCA`
  
* Converting continuous to discrete
  * `Bucketizer`
  * `QuantileDiscretizer`
  
* Many more
  * Spark 2.4.7: http://spark.apache.org/docs/2.4.7/ml-guide.html
  * Spark 3.1.1: http://spark.apache.org/docs/3.1.1/ml-guide.html  

---

## `MLlib` algorithms for machine learning models need **single, numeric features column** as input

Each row in this column contains a vector of data points corresponding to the set of features used for prediction.

* **Use the `VectorAssember` transformer to create a single vector column from a list of columns.**

* **All categorical data needs to be numeric for machine learning**

--

```
# Example from Spark docs
from pyspark.ml.linalg import Vectors
from pyspark.ml.feature import VectorAssembler

dataset = spark.createDataFrame(
    [(0, 18, 1.0, Vectors.dense([0.0, 10.0, 0.5]), 1.0)],
    ["id", "hour", "mobile", "userFeatures", "clicked"])

assembler = VectorAssembler(
    inputCols=["hour", "mobile", "userFeatures"],
    outputCol="features")

output = assembler.transform(dataset)
print("Assembled columns 'hour', 'mobile', 'userFeatures' to vector column 'features'")
output.select("features", "clicked").show(truncate=False)
```

---

## Estimators

.pull-left[
`Estimators` learn (or "fit") parameters from your DataFrame via the `.fit()` method, and return a **model** which is a `Transformer`
]

.pull-right[
&lt;img src="img/spdg_2502.png"&gt;
]

---

## Pipelines

.center[
&lt;img src="img/pipeline-oriented.png" width=800&gt;
]


---
## Pipelines

.pull-left[
`Pipelines` combine multiple steps into a single workflow that can be easily run.
* Data cleaning and feature processing via transformers, using `stages`
* Model definition
* Run the pipeline to do all transformations and fit the model

The `Pipeline` constructor takes an _array_ of pipeline stages
]

.pull-right[
&lt;img src="img/spdg_2404.png"&gt;
]

---

## Why Pipelines?

1. **Cleaner Code:** Accounting for data at each step of preprocessing can get messy. With a Pipeline, you wonâ€™t need to manually keep track of your training and validation data at each step.

1. **Fewer Bugs:** There are fewer opportunities to misapply a step or forget a preprocessing step.

1. **Easier to Productionize:** It can be surprisingly hard to transition a model from a prototype to something deployable at scale, but Pipelines can help.

1. **More Options for Model Validation:** We can easily apply cross-validation and other techniques to our Pipelines.

---

## Example

Using the [HMP Dataset](https://github.com/wchill/HMP_Dataset). The structure of the dataset looks like this:


```python
# read data from text file and split each line into words
df.printSchema()

root  
|-- x: integer (nullable = true)  
|-- y: integer (nullable = true)  
|-- z: integer (nullable = true)  
|-- class: string (nullable = false)  
|-- source: string (nullable = false)
```

---

### Let's transform strings to numbers

* The `StringIndexer` is an _estimator_ having both `fit` and `transform` methods. 
* To create a StringIndexer object (indexer), pass the `class` column as inputCol, and `classIndex` as outputCol. 
* Then we fit the DataFrame to the indexer (to run the estimator), and transform the DataFrame. This creates a brand new DataFrame (indexed), which we can see below, containing the classIndex additional column.



```python
from pyspark.ml.feature import StringIndexer

indexer = StringIndexer(inputCol = 'class', outputCol = 'classIndex')
indexed = indexer.fit(df).transform(df)  # This is a new data frame

# Let's see it
indexed.show(5)

+---+---+---+-----------+--------------------+----------+
|  x|  y|  z|      class|              source|classIndex|
+---+---+---+-----------+--------------------+----------+
| 29| 39| 51|Drink_glass|Accelerometer-201...|       2.0|
| 29| 39| 51|Drink_glass|Accelerometer-201...|       2.0|
| 28| 38| 52|Drink_glass|Accelerometer-201...|       2.0|
| 29| 37| 51|Drink_glass|Accelerometer-201...|       2.0|
| 30| 38| 52|Drink_glass|Accelerometer-201...|       2.0|
+---+---+---+-----------+--------------------+----------+
only showing top 5 rows
```

---

### One-Hot Encode categorical variables

* Unlike the `StringIndexer`, `OneHotEncoder` is a pure _transformer_, having only the `transform` method. It uses the same syntax of `inputCol` and `outputCol`.
* `OneHotEncoder` creates a brand new DataFrame (encoded), with a `category_vec` column added to the previous DataFrame(indexed).
* `OneHotEncoder` doesn't return several columns containing only zeros and ones; it returns a sparse-vector as seen in the categoryVec column. Thus, for the â€˜Drink_glassâ€™ class above, SparkML returns a sparse-vector that basically says there are 13 elements, and at position 2, the class value exists(1.0).



```python
from pyspark.ml.feature import OneHotEncoder

# The OneHotEncoder is a pure transformer object. it does not use the fit()
encoder = OneHotEncoder(inputCol = 'classIndex', outputCol = 'categoryVec')
encoded = encoder.transform(indexed)  # This is a new data frame
encoded.show(5, False)

+---+---+---+-----------+----------------------------------------------------+----------+--------------+
|x  |y  |z  |class      |source                                              |classIndex|categoryVec   |
+---+---+---+-----------+----------------------------------------------------+----------+--------------+
|29 |39 |51 |Drink_glass|Accelerometer-2011-06-01-14-13-57-drink_glass-f1.txt|2.0       |(13,[2],[1.0])|
|29 |39 |51 |Drink_glass|Accelerometer-2011-06-01-14-13-57-drink_glass-f1.txt|2.0       |(13,[2],[1.0])|
|28 |38 |52 |Drink_glass|Accelerometer-2011-06-01-14-13-57-drink_glass-f1.txt|2.0       |(13,[2],[1.0])|
|29 |37 |51 |Drink_glass|Accelerometer-2011-06-01-14-13-57-drink_glass-f1.txt|2.0       |(13,[2],[1.0])|
|30 |38 |52 |Drink_glass|Accelerometer-2011-06-01-14-13-57-drink_glass-f1.txt|2.0       |(13,[2],[1.0])|
+---+---+---+-----------+----------------------------------------------------+----------+--------------+
only showing top 5 rows
```

---

### Assembling the feature vector

* The `VectorAssembler` object gets initialized with the same syntax as `StringIndexer` and `OneHotEncoder`. 
* The list `['x', 'y', 'z']` is paseed to `inputCols`, and we specify outputCol = â€˜featuresâ€™. 
* This is also a pure _transformer_.



```python
from pyspark.ml.linalg import Vectors
from pyspark.ml.feature import VectorAssembler
# VectorAssembler creates vectors from ordinary data types for us

vectorAssembler = VectorAssembler(inputCols = ['x','y','z'], outputCol = 'features')
# Now we use the vectorAssembler object to transform our last updated dataframe

features_vectorized = vectorAssembler.transform(encoded)  # note this is a new df

features_vectorized.show(5, False)

+---+---+---+-----------+----------------------------------------------------+----------+--------------+----------------+
|x  |y  |z  |class      |source                                              |classIndex|categoryVec   |features        |
+---+---+---+-----------+----------------------------------------------------+----------+--------------+----------------+
|29 |39 |51 |Drink_glass|Accelerometer-2011-06-01-14-13-57-drink_glass-f1.txt|2.0       |(13,[2],[1.0])|[29.0,39.0,51.0]|
|29 |39 |51 |Drink_glass|Accelerometer-2011-06-01-14-13-57-drink_glass-f1.txt|2.0       |(13,[2],[1.0])|[29.0,39.0,51.0]|
|28 |38 |52 |Drink_glass|Accelerometer-2011-06-01-14-13-57-drink_glass-f1.txt|2.0       |(13,[2],[1.0])|[28.0,38.0,52.0]|
|29 |37 |51 |Drink_glass|Accelerometer-2011-06-01-14-13-57-drink_glass-f1.txt|2.0       |(13,[2],[1.0])|[29.0,37.0,51.0]|
|30 |38 |52 |Drink_glass|Accelerometer-2011-06-01-14-13-57-drink_glass-f1.txt|2.0       |(13,[2],[1.0])|[30.0,38.0,52.0]|
+---+---+---+-----------+----------------------------------------------------+----------+--------------+----------------+
only showing top 5 rows
```

---

### Normalizing the dataset 

* `Normalizer` like all transformers have consistent syntax. Looking at the `Normalizer` object, it contains the parameter `p=1.0` (the default norm value for Pyspark Normalizer is `p=2.0`.)
* `p=1.0` uses Manhattan Distance
* `p=2.0` uses Euclidean Distance


```python
from pyspark.ml.feature import Normalizer
normalizer = Normalizer(inputCol = 'features', outputCol = 'features_norm', p=1.0)  # Manhattan Distance
normalized_data = normalizer.transform(features_vectorized) # New data frame too.

normalized_data.show(5)
&gt;&gt;
+---+---+---+-----------+--------------------+----------+--------------+----------------+--------------------+
|  x|  y|  z|      class|              source|classIndex|   categoryVec|        features|       features_norm|
+---+---+---+-----------+--------------------+----------+--------------+----------------+--------------------+
| 29| 39| 51|Drink_glass|Accelerometer-201...|       2.0|(13,[2],[1.0])|[29.0,39.0,51.0]|[0.24369747899159...|
| 29| 39| 51|Drink_glass|Accelerometer-201...|       2.0|(13,[2],[1.0])|[29.0,39.0,51.0]|[0.24369747899159...|
| 28| 38| 52|Drink_glass|Accelerometer-201...|       2.0|(13,[2],[1.0])|[28.0,38.0,52.0]|[0.23728813559322...|
| 29| 37| 51|Drink_glass|Accelerometer-201...|       2.0|(13,[2],[1.0])|[29.0,37.0,51.0]|[0.24786324786324...|
| 30| 38| 52|Drink_glass|Accelerometer-201...|       2.0|(13,[2],[1.0])|[30.0,38.0,52.0]|[0.25,0.316666666...|
+---+---+---+-----------+--------------------+----------+--------------+----------------+--------------------+
only showing top 5 rows
```



---


### Running the transformation `Pipeline`

The `Pipeline` constructor below takes an array of Pipeline stages we pass to it. Here we pass the 4 stages defined earlier, in the right sequence, one after another.


```python
from pyspark.ml import Pipeline
pipeline = Pipeline(stages = [indexer,encoder,vectorAssembler,normalizer])
```

Now the `Pipeline` object fit to our original `DataFrame df` 


```python
data_model = pipeline.fit(df)
```

Finally, we transform our DataFrame `df` using the Pipeline Object.


```python
pipelined_data = data_model.transform(df)

pipelined_data.show(5)

+---+---+---+-----------+--------------------+----------+--------------+----------------+--------------------+
|  x|  y|  z|      class|              source|classIndex|   categoryVec|        features|       features_norm|
+---+---+---+-----------+--------------------+----------+--------------+----------------+--------------------+
| 29| 39| 51|Drink_glass|Accelerometer-201...|       2.0|(13,[2],[1.0])|[29.0,39.0,51.0]|[0.24369747899159...|
| 29| 39| 51|Drink_glass|Accelerometer-201...|       2.0|(13,[2],[1.0])|[29.0,39.0,51.0]|[0.24369747899159...|
| 28| 38| 52|Drink_glass|Accelerometer-201...|       2.0|(13,[2],[1.0])|[28.0,38.0,52.0]|[0.23728813559322...|
| 29| 37| 51|Drink_glass|Accelerometer-201...|       2.0|(13,[2],[1.0])|[29.0,37.0,51.0]|[0.24786324786324...|
| 30| 38| 52|Drink_glass|Accelerometer-201...|       2.0|(13,[2],[1.0])|[30.0,38.0,52.0]|[0.25,0.316666666...|
+---+---+---+-----------+--------------------+----------+--------------+----------------+--------------------+
only showing top 5 rows
```

---

### One last step before training


```python
# first let's list out the columns we want to drop
cols_to_drop = ['x','y','z','class','source','classIndex','features']

# Next let's use a list comprehension with conditionals to select cols we need
selected_cols = [col for col in pipelined_data.columns if col not in cols_to_drop]

# Let's define a new train_df with only the categoryVec and features_norm cols
df_train = pipelined_data.select(selected_cols)

# Let's see our training dataframe.
df_train.show(5)

+--------------+--------------------+
|   categoryVec|       features_norm|
+--------------+--------------------+
|(13,[2],[1.0])|[0.24369747899159...|
|(13,[2],[1.0])|[0.24369747899159...|
|(13,[2],[1.0])|[0.23728813559322...|
|(13,[2],[1.0])|[0.24786324786324...|
|(13,[2],[1.0])|[0.25,0.316666666...|
only showing top 5 rows
```

### You now have a DataFrame optimized for SparkML!

---

## Machine Learning Models in Spark

There are many [Spark machine learning models](https://spark.apache.org/docs/3.1.1/ml-classification-regression.html)

#### Regression
- Linear regression - what is the optimization method?

--

- Survival regression
- Genearlized linear model (GLM)
- Random forest regression


#### Classification
- Logistic regression
- Gradient-boosted tree model (GBM)
- Naive Bayes
- Multilayer perception (neural network!)

#### Other
- Clustering (K-means, LDA, GMM)
- Association rule mining

---

## Building your Model

Feature selection using easy R-like formulas

```
# Example from Spark docs
from pyspark.ml.feature import RFormula

dataset = spark.createDataFrame(
    [(7, "US", 18, 1.0),
     (8, "CA", 12, 0.0),
     (9, "NZ", 15, 0.0)],
    ["id", "country", "hour", "clicked"])

formula = RFormula(
    formula="clicked ~ country + hour",
    featuresCol="features",
    labelCol="label")

output = formula.fit(dataset).transform(dataset)
output.select("features", "label").show()
```
---

## What if you have too many columns for your model?

Evaluating 1,000s or 10,000s of variables?

---

## Chi-Squared Selector

Pick categorical variables that are most dependent on the response variable

Can even check the [p-values of specific variables!](https://stackoverflow.com/questions/50971964/pyspark-chisqselector-p-values-and-test-statistics)

```
# Example from Spark docs
from pyspark.ml.feature import ChiSqSelector
from pyspark.ml.linalg import Vectors

df = spark.createDataFrame([
    (7, Vectors.dense([0.0, 0.0, 18.0, 1.0]), 1.0,),
    (8, Vectors.dense([0.0, 1.0, 12.0, 0.0]), 0.0,),
    (9, Vectors.dense([1.0, 0.0, 15.0, 0.1]), 0.0,)], ["id", "features", "clicked"])

selector = ChiSqSelector(numTopFeatures=1, featuresCol="features",
                         outputCol="selectedFeatures", labelCol="clicked")

result = selector.fit(df).transform(df)

print("ChiSqSelector output with top %d features selected" % selector.getNumTopFeatures())
result.show()
```

---

## Tuning your model

#### Part 1

```
# Example from Spark docs
from pyspark.ml.evaluation import RegressionEvaluator
from pyspark.ml.regression import LinearRegression
from pyspark.ml.tuning import ParamGridBuilder, TrainValidationSplit

# Prepare training and test data.
data = spark.read.format("libsvm")\
    .load("data/mllib/sample_linear_regression_data.txt")
train, test = data.randomSplit([0.9, 0.1], seed=12345)

lr = LinearRegression(maxIter=10)

# We use a ParamGridBuilder to construct a grid of parameters to search over.
# TrainValidationSplit will try all combinations of values and determine best model using
# the evaluator.
paramGrid = ParamGridBuilder()\
    .addGrid(lr.regParam, [0.1, 0.01]) \
    .addGrid(lr.fitIntercept, [False, True])\
    .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0])\
    .build()

```

---

## Tuning your model

#### Part 2

```
# In this case the estimator is simply the linear regression.
# A TrainValidationSplit requires an Estimator, a set of Estimator ParamMaps, and an Evaluator.
tvs = TrainValidationSplit(estimator=lr,
                           estimatorParamMaps=paramGrid,
                           evaluator=RegressionEvaluator(),
                           # 80% of the data will be used for training, 20% for validation.
                           trainRatio=0.8)

# Run TrainValidationSplit, and choose the best set of parameters.
model = tvs.fit(train)

# Make predictions on test data. model is the model with combination of parameters
# that performed best.
model.transform(test)\
    .select("features", "label", "prediction")\
    .show()
```
]
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"ratio": "16:9",
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
